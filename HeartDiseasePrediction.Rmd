---
title: "Heart Disease Prediction"
author: "Syed Danish Ahmed"
date: "December 2019"
output: 
  html_document:
    theme: readable
    highlight: tango
---

<p>&nbsp;</p>
<p>&nbsp;</p>

## Introduction

<p>&nbsp;</p>

### What is a Heart Disease

The term “heart disease” refers to several types of heart conditions. The medical definition includes any disorder that affects the heart. Sometimes the term "heart disease" is used narrowly and incorrectly as a synonym for coronary artery disease. Heart disease is synonymous with cardiac disease but not with cardiovascular disease which is any disease of the heart or blood vessels.

<p>&nbsp;</p>

### Facts about Heart Diseases

* Heart diseases remain the leading cause of death in the United States, responsible for 840,768 deaths (635,260 cardiac) in 2016. From 2006 to 2016, the US death rate from CVD decreased by 18.6% and from coronary heart disease by 31.8%
* In a survey, 92% respondents recognized chest pain as a symptom of a heart attack. Only 27% were aware of all major symptoms and knew to call 9-1-1 when someone was having a heart attack
* About 47% of sudden cardiac deaths occur outside a hospital. This suggests that many people with heart disease don’t act on early warning signs

*Source: https://www.cdc.gov/heartdisease/facts.htm*

Many forms of heart disease can be prevented or treated with healthy lifestyle choices. Here we will perform an analysis to understand the health metrics that are associated with the presence and absence of heart diseases. 

<p>&nbsp;</p>

### Kaggle Heart Diseases Dataset
The dataset on which we will be carrying out the analysis pertaining to heart diseases has been obtained from Kaggle - https://www.kaggle.com/ronitf/heart-disease-uci. The dataset contains various heart health metrics corresponding for individuals with and without a heart disease. All types of heart diseases are being considered as one. 

<p>&nbsp;</p>

## Steps involved in this analysis are as below

* Defining the objective of the analysis
* Getting an initial understanding of the dataset and missing value treatment
* Performing exploratory data analysis to summarizing the data characteristics
* Carrying out statistical analyses for identifying features of importance
* Training and validating a base classification model with all features
* Evaluating the base model on unseen test data points
* Training and validating an improved classification model with only the important features
* Evaluating the improved model on unseen test data points
* Generating conclusions from the analysis

<p>&nbsp;</p>

## Objectives of the analysis

Early action is important for averting potential risks posed by heart diseases. Hence it is important to be aware about the possibility of having a disease so that corrective actions can be taken before it is too late. 

The objective of the analysis is thus to identify the health metrics associated with heart diseases and build a classification model to predict whether an individual with given health metrics is likely to have a heart disease or not. 

<p>&nbsp;</p>

## Initial understanding of the underlying data

Following is some information about the dataset obtained from the source.

1. *age*: Age of the individual in years
2. *sex*: Sex of the individual. 1 = male, 0 = female
3. *cp*: Chest pain type (4 values)
4. *trestbps*: Resting blood pressure (in mm Hg on admission to the hospital)
5. *chol*: Serum cholestoral in mg/dl
6. *fbs*: Fasting blood sugar > 120 mg/dl. 1 = male, 0 = female
7. *restecg*: Resting electrocardiographic results (values 0,1,2)
8. *thalach*: Maximum heart rate achieved
9. *exang*: Exercise induced angina. 1 = yes, 0 = no
10. *oldpeak*: ST depression induced by exercise relative to rest
11. *slope*: The slope of the peak exercise ST segment
12. *ca*: Number of major vessels (0-3) colored by flourosopy
13. *thal*: 3 = normal, 6 = fixed defect, 7 = reversable defect
14. *target*: 0 = With heart disease, 1 = Without heart disease

<p>&nbsp;</p>

We will now import the data and see what it looks like.

```{r}
# Ensure that the results are reproducible
set.seed(0)
# Importing the dataset from csv file
filename = "heart.csv"
heart.disease.data <- read.csv(filename, header=TRUE, sep=",")
head(heart.disease.data, 10)
```

<p>&nbsp;</p>

We see that the first column 'age' has some special characters, hence updating the column name.


```{r}
# Updating the column name for age
colnames(heart.disease.data)[1] <- "age"
head(heart.disease.data, 10)
```
  
<p>&nbsp;</p>

Checking for missing values in the dataset.


```{r}
# Checking for NAs in the dataset
apply(heart.disease.data, 2, function(x) any(is.na(x)))
```


As we can see above, none of the columns have any missing values.

<p>&nbsp;</p>

## Exploring the dataset

Let's look at the data further to get a better understanding of its characteristics. 

> There are `r nrow(heart.disease.data)` rows and `r ncol(heart.disease.data)` columns in the dataset 

<p>&nbsp;</p>

### Structure of the dataset
```{r}
# Structure of the dataset
str(heart.disease.data)
```

We see that the data types of several variables like sex, cp, fbs, restecg, exang, slope, ca, thal and target, which should be categorical are also being treated as integer. Hence, the next step is to update the data types.

<p>&nbsp;</p>

```{r}
# Updating the data types of variables
heart.disease.data <- transform(
  heart.disease.data,
  age = as.integer(age),
  sex = as.factor(sex),
  cp = as.factor(cp),
  trestbps = as.integer(trestbps),
  chol = as.integer(chol),
  fbs = as.factor(fbs),
  restecg = as.factor(restecg),
  thalach = as.integer(thalach),
  exang = as.factor(exang),
  oldpeak = as.numeric(oldpeak),
  slope = as.factor(slope),
  ca = as.factor(ca),
  thal = as.factor(thal),
  target = as.factor(target)
)

# Getting the class corresponding to each variable
sapply(heart.disease.data, class)
```

The data types have now been updated. 

<p>&nbsp;</p>

### Summarizing the variables in the dataset
```{r}
# Summarizing the variables in the dataset
summary(heart.disease.data)
```

Here we are able to view the Descriptive Statistics results of the dataset.
<p>&nbsp;</p>


### Visualizations and Hypothesis Testing for Feature Importance




#### Age


We will start with looking at the age of all the individuals in the dataset

```{r}
library(ggplot2)

hist(heart.disease.data$age, 
     main="Histogram for age of individuals", 
     xlab="Age", 
     border="green", 
     col="darkblue",
     breaks=10
     )
```

We can see that the distribution is nearly normal. We can now try to see if the age can be bucketed 

<p>&nbsp;</p>

```{r}

# Bucketing the population based on age
youngsters <- heart.disease.data[which((heart.disease.data$age < 45)), ]
middleaged <- heart.disease.data[which((heart.disease.data$age >= 45)&(heart.disease.data$age < 60)), ]
eldercitizens <- heart.disease.data[which(heart.disease.data$age > 60), ]
buckets <- data.frame(age.buckets = c("youngsters","middleaged","eldercitizens"), bucket.count = c(NROW(youngsters$age), NROW(middleaged$age), NROW(eldercitizens$age)))

# Creating bar plots for age groups
ggplot(buckets, aes(x=buckets$age.buckets, y=buckets$bucket.count, fill=buckets$age.buckets)) + 
  ggtitle("Bucketing age groups for analysis") +
  xlab("Age Buckets")  +
  ylab("Count") +
  geom_bar(stat="identity") +
  scale_fill_discrete(name = "Age Buckets")
```

After creating the buckets we can see that the number of people in the middle bucket are the highest.

Now to get a sense of how the distribution of each variable differs across the groups with and without heart diseases, we will juxtapose the two distributions for each variable.

<p>&nbsp;</p>

```{r}
library(ggplot2)

# Comparing distribution of age
g <- ggplot(heart.disease.data, aes(age))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of age for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Age")
```

We see that the distribution of age for group with heart diseases is left-skewed whereas it is multimodal for the group without heart diseases.

<p>&nbsp;</p>

```{r}
# Comparing plots for age
ggplot(data = heart.disease.data, mapping = aes(x = target, y = age, group = target)) + geom_boxplot(fill='grey',color="blue") +
  ggtitle("Age for heart disease group and non-heart disease group") +
  labs(y="Age", x = "Target")
```

We see that the median age of the group with heart disease is higher than those without heart diseases. 

Now in order to see whether the target is dependent on this feature, we will perform a t-test.

<p>&nbsp;</p>

```{r}
t.out <- t.test(heart.disease.data$age ~ heart.disease.data$target, conf.level = 0.95)
print(t.out)
```


Since the t-statistic is greater than 1.96, we can say that the values of age for target 1 are significantly different from values of age for target 0. Hence, this feature is significantly important in predicting the target.


<p>&nbsp;</p>

#### Sex


```{r}
# Comparing distribution of gender
library(ggplot2)
g <- ggplot(heart.disease.data, aes(sex))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of gender for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Sex")
```


We see that the difference in the number of males and females is higher for group with heart diseases when compared to the group without heart diseases.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.


<p>&nbsp;</p>

```{r}
chisq.test(table(heart.disease.data$sex, heart.disease.data$target), correct = FALSE)
```

Since the p value is greater than 0.05, we do not have sufficient evidence to reject the null hypothesis that the two variables are independent. Hence, this feature is not significantly important in predicting the target.


<p>&nbsp;</p>

#### cp - Chest Pain


```{r}
# Comparing distribution of cp
library(ggplot2)
g <- ggplot(heart.disease.data, aes(cp))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Chest Pain for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Chest Pain")
```

We see that the difference in frequencies across different cp values for the groups with and without heart diseases is high for values 0, 1 and 2.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.

<p>&nbsp;</p>

```{r}
chisq.test(table(heart.disease.data$cp, heart.disease.data$target), correct = FALSE)
```

Since the p value is less than 0.05, we have sufficient evidence to reject the null hypothesis that the two variables are independent. Hence, this feature is significantly important in predicting the target.


<p>&nbsp;</p>

#### trestbps - Resting Blood Pressure


```{r}
# Comparing distribution of trestbps
library(ggplot2)
g <- ggplot(heart.disease.data, aes(trestbps))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Resting Blood Pressure for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Resting Blood Pressure")
```

We see that both the distributions are multimodal.

<p>&nbsp;</p>

```{r}
# Comparing plots for trestbps
ggplot(data = heart.disease.data, mapping = aes(x = target, y = trestbps, group = target)) + geom_boxplot(fill='grey',color="blue") +
  ggtitle("Resting Blood Pressure for heart disease group and non-heart disease group") +
  labs(y="Resting Blood Pressure", x = "Target")
```


We see that the median for both the groups is nearly equal. 


Now in order to see whether the target is dependent on this feature, we will perform a t-test.


<p>&nbsp;</p>

```{r}
t.out <- t.test(heart.disease.data$trestbps ~ heart.disease.data$target, conf.level = 0.95)
print(t.out)
```


Since the t-statistic is greater than 1.96, we can say that the values of trestbps for target 1 are significantly different from values of age for target 0. Hence, this feature is significantly important in predicting the target.


<p>&nbsp;</p>

#### chol - Cholesterol 

```{r}
# Comparing distribution of chol
library(ggplot2)
g <- ggplot(heart.disease.data, aes(chol))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Cholesterol levels for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Cholesterol")
```


We see that both the distributions are sort of right-skewed.

<p>&nbsp;</p>

```{r}
# Comparing plots for chol
ggplot(data = heart.disease.data, mapping = aes(x = target, y = chol, group = target)) + geom_boxplot(fill='grey',color="blue") +
  ggtitle("Cholesterol levels for heart disease group and non-heart disease group") +
  labs(y="Cholesterol", x = "Target")
```

We see that the median for both the groups is nearly equal, the non-heart disease group being a little lower 


Now in order to see whether the target is dependent on this feature, we will perform a t-test.

<p>&nbsp;</p>

```{r}
t.out <- t.test(heart.disease.data$chol ~ heart.disease.data$target, conf.level = 0.95)
print(t.out)
```


Since the t-statistic is less than 1.96, we fail to reject the null hypothesis. We can say that the values of chol for target 1 are not significantly different from values of age for target 0. Hence, this feature is not significantly important in predicting the target.


<p>&nbsp;</p>

#### fbs - Fasting Blood Sugar


```{r}
# Comparing distribution of fbs
library(ggplot2)
g <- ggplot(heart.disease.data, aes(fbs))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Fasting Blood Sugar for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Fasting Blood Sugar")
```

We see that the difference in fbs values for the groups with and without heart diseases is not much.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.


<p>&nbsp;</p>


```{r}
chisq.test(table(heart.disease.data$fbs, heart.disease.data$target), correct = FALSE)
```

Since the p value is greater than 0.05, we do not have have sufficient evidence to reject the null hypothesis that the two variables are independent. Hence, this feature is not significantly important in predicting the target.


<p>&nbsp;</p>

#### restecg - Resting Electrocardiographic Measurement


```{r}
# Comparing distribution of restecg
library(ggplot2)
g <- ggplot(heart.disease.data, aes(restecg))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Resting Electrocardiographic Measurement for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Resting Electrocardiographic Measurement")
```

We see that the difference in restecg values for the groups with and without heart diseases is not much.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.

```{r}
chisq.test(table(heart.disease.data$restecg, heart.disease.data$target), correct = FALSE)
```

It gave the warning because many of the expected values will be very small and therefore the approximations of p may not be right. Hence, we cannot come to a definitive conclusion for this feature.


<p>&nbsp;</p>

#### thalach - Maximum Heart Rate Achieved


```{r}
# Comparing distribution of thalach
library(ggplot2)
g <- ggplot(heart.disease.data, aes(thalach))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Maximum Heart Rate Achieved for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Maximum Heart Rate Achieved")
```

We see that the distribution of thalach is multimodal for both the groups.


<p>&nbsp;</p>

```{r}
# Comparing plots for thalach
ggplot(data = heart.disease.data, mapping = aes(x = target, y = thalach, group = target)) + geom_boxplot(fill='grey',color="blue") +
  ggtitle("Maximum Heart Rate Achieved for heart disease group and non-heart disease group") +
  labs(y="Maximum Heart Rate Achieved", x = "Target")
```

We see that the median for the group without heart diseases is higher. 


Now in order to see whether the target is dependent on this feature, we will perform a t-test.

<p>&nbsp;</p>


```{r}
t.out <- t.test(heart.disease.data$thalach ~ heart.disease.data$target, conf.level = 0.95)
print(t.out)
```


Since the t-statistic is less than -1.96, we can say that the values of thalach for target 1 are significantly different from values of age for target 0. Hence, this feature is significantly important in predicting the target.


<p>&nbsp;</p>

#### exang - Exercise Induced Angina


```{r}
# Comparing distribution of exang
library(ggplot2)
g <- ggplot(heart.disease.data, aes(exang))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Exercise Induced Angina for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Exercise Induced Angina")
```

We see that the differences in frequencies corresponding to each of the values of exang for the groups with and without heart diseases is different based on the exang value. The difference is higher for value 0.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.

<p>&nbsp;</p>

```{r}
chisq.test(table(heart.disease.data$exang, heart.disease.data$target), correct = FALSE)
```

Since the p value is greater than 0.05, we do not have sufficient evidence to reject the null hypothesis that the two variables are independent. Hence, this feature is not significantly important in predicting the target.


<p>&nbsp;</p>

#### oldpeak
 

```{r}
# Comparing distribution of oldpeak
library(ggplot2)
g <- ggplot(heart.disease.data, aes(oldpeak))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of oldpeak for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "oldpeak")
```

We see that the distribution of oldpeak for both the groups is multimodal.

<p>&nbsp;</p>

```{r}
# Comparing plots for oldpeak
ggplot(data = heart.disease.data, mapping = aes(x = target, y = oldpeak, group = target)) + geom_boxplot(fill='grey',color="blue") +
  ggtitle("oldpeak for heart disease group and non-heart disease group") +
  labs(y="oldpeak", x = "Target")
```


We see that the median for the group with heart disease is higher. 


Now in order to see whether the target is dependent on this feature, we will perform a t-test.

<p>&nbsp;</p>

```{r}
t.out <- t.test(heart.disease.data$oldpeak ~ heart.disease.data$target, conf.level = 0.95)
print(t.out)
```


Since the t-statistic is greater than 1.96, we can say that the values of trestbps for target 1 are significantly different from values of age for target 0. Hence, this feature is significantly important in predicting the target.


<p>&nbsp;</p>

#### slope


```{r}
# Comparing distribution of slope
library(ggplot2)
g <- ggplot(heart.disease.data, aes(slope))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of slope for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "slope")
```


We see that the differences in frequencies corresponding to each of the values of slope for the groups with and without heart diseases is different based on the slope value. The differences are higher for values 1 and 2.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.

<p>&nbsp;</p>

```{r}
chisq.test(table(heart.disease.data$slope, heart.disease.data$target), correct = FALSE)
```

Since the p value is greater than 0.05, hence we do not have have sufficient evidence to reject the null hypothesis that the two variables are independent. Hence, this feature is not significantly important in predicting the target.


<p>&nbsp;</p>

#### ca - Number of Major Vessels


```{r}
# Comparing distribution of ca
library(ggplot2)
g <- ggplot(heart.disease.data, aes(ca))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Number of Major Vessels for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Number of Major Vessels")
```


We see that the differences in frequencies corresponding to each of the values of ca for the groups with and without heart diseases is different based on the ca value. The difference is higher for value 1.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.

<p>&nbsp;</p>

```{r}
chisq.test(table(heart.disease.data$ca, heart.disease.data$target), correct = FALSE)
```

It gave the warning because many of the expected values will be very small and therefore the approximations of p may not be right. Hence, we cannot come to a definitive conclusion for this feature.


<p>&nbsp;</p>

#### thal - Thalassemia 


```{r}
# Comparing distribution of thal
library(ggplot2)
g <- ggplot(heart.disease.data, aes(thal))
g + geom_bar(aes(group=target), fill='grey',color="darkgreen") + facet_wrap(~target) + theme(legend.position = "none") + 
  ggtitle("Distribution of Thalassemia for heart disease group and non-heart disease group") +
  labs(y="Frequency", x = "Thalassemia")
```


We see that the differences in frequencies corresponding to each of the values of thal for the groups with and without heart diseases is different based on the thal value. The differences are higher for values 2 and 3.

Now in order to see whether the target is dependent on this feature, we will perform a chi-square test since these variables are categorical.

<p>&nbsp;</p>

```{r}
chisq.test(table(heart.disease.data$thal, heart.disease.data$target), correct = FALSE)
```

It gave the warning because many of the expected values will be very small and therefore the approximations of p may not be right. Hence, we cannot come to a definitive conclusion for this feature.


<p>&nbsp;</p>

### Significant factors based on hypothesis testing

After ruling out the factors which did not come out to be significant based of hypothesis testing, we are left with age, cp, trestbps, thalach, oldpeak, restecg, ca and thal. 

Note that we are only removing the ones where were proved to be not significant.

<p>&nbsp;</p>

### Looking at correlations

We will now look at the correlation matrix to determine the correlations between the variables.
Note that we will look at the numeric variables only.

```{r}
library(corrplot)
numeric.variables <- unlist(lapply(heart.disease.data, is.numeric))  
heart.disease.data.numeric <- heart.disease.data[ , numeric.variables]

corrdata <- cor(heart.disease.data.numeric)
print(corrdata)
corrplot(corrdata)
```

We see that the highest correlation value is 0.4 which is not strong enough. Hence, we will not remove any of the significant variables based on multicollinearity. 

<p>&nbsp;</p>

## Building the classification model


### Check for class imbalance

Class Imbalance means that the number of data points available for the different classes is different. If there are two classes, then balanced data would mean nearly 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation.

```{r}
# Bar plot for target (Heart disease) 
ggplot(heart.disease.data, aes(x=heart.disease.data$target, fill = heart.disease.data$target)) + 
  geom_bar() +
  xlab("Target") +
  ylab("Frequency") +
  ggtitle("Comparing data points for the classes") +
  scale_fill_discrete(name = "Heart Disease", labels = c("Group without heart disease", "Group with heart disease"))

# Getting the raw numbers
table(heart.disease.data$target) 
```

The difference in the number of data points for the two classes is 19.56%, which is acceptable and so there is no need to balance the classes.

<p>&nbsp;</p>

### Creating the train test split

We are divided the entire dataset into two groups, one for training the model and the other for testing the model.

```{r}
library(caret)
set.seed(1120)
#partition the data into a 70%/30% training/testing split
inTrain <- createDataPartition(y = heart.disease.data$target,
                               p = .7,
                               list = FALSE)
training <- heart.disease.data[inTrain, ] # training dataframe
testing <- heart.disease.data[-inTrain, ] # testing dataframe
```

createDataPartition does a stratified random split of the data, hence we do not need to shuffle the data.

We will now use trainConrol() to determine the resampling method.

<p>&nbsp;</p>

### Building the base classification model using all the features

We will first create a base model using all the features in the data. This will be used as a reference against the improved model which will only have the significant features that we had arrived upon using hypothesis testing.

<p>&nbsp;</p>

#### Boosted Logistic Regression - base model with all features

This model contains all the features.

```{r}
# Define train control parameters
ctrl <- trainControl(method="repeatedcv", 
                     repeats=10)

# Define parameters to be used for lr model
blrFit <- train(target ~ .,
                data=training,
                method="LogitBoost",
                trControl=ctrl)

# Using the trained model to predict on test data
blrClasses <- predict(blrFit, newdata = testing)

# Performance measurement
postResample(blrClasses, testing$target)

# Confusion matrix
cmblr <-confusionMatrix(blrClasses, testing$target)
print(cmblr)
```


We see that the accuracy of this base model is 82.22 %
Kappa is 64.72 %

<p>&nbsp;</p>


#### Boosted Logistic Regression - Improved model with only the significant features

This model contains only the significant features.

```{r}
#define parameters to be used for lr model
blrFit <- train(target ~ age + cp + trestbps + thalach + oldpeak + restecg + ca + thal,
                data=training,
                method="LogitBoost",
                trControl=ctrl)

# Using the trained model to predict on test data
blrClasses <- predict(blrFit, newdata = testing)

# Performance measurement
postResample(blrClasses, testing$target)

# Confusion matrix
cmblr <-confusionMatrix(blrClasses, testing$target)
print(cmblr)

```


We see that the accuracy of this base model is 85.56 %
Kappa is 70.94 %

Hence, we are getting a better performing model when train the model only on the significant features. Training on lesser number of features also makes the compuation faster and less resource intensive, which in turn creates better model scaling outlook. 

<p>&nbsp;</p>

### Conclusions

In this analysis, we identified the major factors associated with heart diseases. The identified factors are age, chest pain, resting bloos pressure, maximum heart rate achieved, oldpeak, resting electrocardiographic measurement, number of major vessels and thalassemia.

We also created a predictive model using Boosted Logistic Regression which has a test accuracy of 85.56 %. This model was based on the selected features and it outperformed the base model which was trained on all the features.

<p>&nbsp;</p>

### References
* Heart Disease
  + https://www.mayoclinic.org/diseases-conditions/heart-disease/symptoms-causes/syc-20353118

* Facts about heart diseases
  + https://www.cdc.gov/heartdisease/facts.htm
  + https://www.acc.org/latest-in-cardiology/ten-points-to-remember/2019/02/15/14/39/aha-2019-heart-disease-and-stroke-statistics